general:
  name:
#  scenario: 'point_robot_easy'
#  scenario: 'point_robot_easy2'
#  scenario: 'point_robot_easy2_transposed'
#  scenario: 'point_robot_box_0.2'
#  scenario: 'point_robot_box_0.8'
#  scenario: 'point_robot_box_1.6'
#  scenario: 'point_robot_corridor'
#  scenario: 'point_robot_hard_corridors'
  scenario: panda_no_obs_fixed_start
  #  scenario: panda_no_obs
  #  scenario: panda_easy
  #  scenario: panda_easy_fixed_start
  #  scenario: panda_hard
  #  scenario: panda_poles
  gpu_usage: 0.1
  actor_gpu_usage: 0.05
  training_cycles: 30000
#  train_episodes_per_cycle: 5000
  train_episodes_per_cycle: 100
#  train_episodes_per_cycle: 5
#  test_episodes: 1
  test_episodes: 100
  save_every_cycles: 1000000
  cycles_per_trajectory_print: 1000000
  test_frequency: 3

panda_game:
  limit_workers:
#  limit_workers: 30
#  limit_workers: 1
  goal_closeness_distance: 0.1
#  limit_action_distance:
  limit_action_distance: 0.1
  move: 'single-action'
#  move: 'multi-action-smooth'
  max_steps: 1000
  add_distance_to_failed: True
#  add_distance_to_failed: False
  add_distance_to_keep_alive: True
#  add_distance_to_keep_alive: False
#  add_distance_to_success: True
  add_distance_to_success: False

cost:
  collision_cost: 1000.0
  goal_reward: 6000.
  is_constant_collision_cost: False
  free_cost: 1.0
  is_constant_free_cost: False
  type: 'linear'
#  type: 'huber'
#  type: 'square'
  huber_loss_delta: 1.0
  goal_closeness_distance: 0.01

model:
  gamma: 0.95
  batch_size: 32
  reset_best_every: 0
#  reset_best_every: 50
  decrease_learn_rate_if_static_success: 100
  restore_on_decrease: True
#  restore_on_decrease: False
  stop_training_after_learn_rate_decrease: 3
#  repeat_train_trajectories:
  repeat_train_trajectories: 10
  epochs: 10

policy:
  learning_rate: 0.0001
#  learning_rate: 0.005
  learning_rate_decrease_rate: 1.
#  learning_rate_decrease_rate: 0.8
  learning_rate_minimum: 0.000025
  gradient_limit: 10.0
#  gradient_limit: 100.0
  gradient_limit_quantile:
#  gradient_limit_quantile: 0.9
  gradient_history_limit: 0
#  gradient_history_limit: 100
#  include_middle_state_as_input: True
  include_middle_state_as_input: False
#  layers: [5, 5]
  layers: [20, 20, 20]
#  activation: 'elu'
#  activation: 'relu'
  activation: 'tanh'
  base_std: 0.05
  decrease_std_every: 50
  std_decrease_rate: 1.0
  learn_std: False
#  learn_std: True
  max_entropy_coefficient: 1.
#  bias_activation_is_tanh: True
  bias_activation_is_tanh: False
#  bias_towards_goal:
  bias_towards_goal: 0.01
  ppo_epsilon: 0.05

value_estimator:
  learning_rate: 0.0005
#  learning_rate: 0.005
  learning_rate_decrease_rate: 1.
#  learning_rate_decrease_rate: 0.8
  learning_rate_minimum: 0.00025
#  gradient_limit: 0.0
  gradient_limit: 1000.0
  gradient_limit_quantile:
#  gradient_limit_quantile: 0.9
  gradient_history_limit: 0
#  gradient_history_limit: 100
#  layers: [5, 5]
#  layers: [50, 50,
  layers: [20, 20, 20]
  activation: 'elu'
#  activation: 'relu'
#  activation: 'tanh'

curriculum:
  use: True
#  use: False
  times_goal_start_coefficient: 1.5
  raise_times: 1.1
  raise_when_train_above: 0.9
